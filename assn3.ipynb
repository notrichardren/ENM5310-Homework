{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "## Problem 1 [20 Points]\n",
    "\n",
    "Consider a Bayesian model with parameters $\\theta$ for which you want to maximize its marginal log-likelihood $\\log p(\\mathcal{D})$ given some observed data $\\mathcal{D}$. As this objective is generally intractable, assume a variational approximation to\n",
    "the posterior by introducing an auxiliary distribution $q(\\theta\\lvert\\mathcal{D})$.\n",
    "\n",
    "(1) Please derive the following inequality\n",
    ":\n",
    "$\\log p(\\mathcal{D}) \\geq \\mathbb{E}_{q(\\theta\\lvert\\mathcal{D})}\\log p(\\mathcal{D}\\lvert \\theta) + \\mathbb{E}_{q(\\theta\\lvert\\mathcal{D})}\\log p(\\theta) - \\mathbb{E}_{q(\\theta\\lvert\\mathcal{D})}\\log q(\\theta\\lvert\\mathcal{D}) $\n",
    "\n",
    "\n",
    "(2) Show that the above inequality becomes an equality when the KL divergence between the true posterior and your variational posterior becomes 0, i.e.\n",
    "\n",
    "$$\\mathbb{KL}[q(\\theta\\lvert\\mathcal{D})||p(\\theta\\lvert \\mathcal{D})] = 0$$\n",
    "\n",
    "(3) One way to design a more flexible variational approximation beyond the mean-field familty is by considering an invertible transformation $f_{\\phi}: \\mathcal{Z} \\to \\mathcal{\\Theta}$, where $\\phi$ denotes the parameters of the invertible transformation.Using the change of variables formula, we can obtain a variational approximation $q_{\\phi}(\\theta | \\mathcal{D})$ that is easy to sample from and easy to evaluate as\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta &= f(z), \\quad \\text{with}\\; z \\sim p_z(z), \\\\\n",
    "    q_{\\phi} (\\theta | \\mathcal{D}) &= p_z(f_{\\phi}^{-1}(\\theta)) | \\mathrm{det} \\nabla_\\theta (f_{\\phi}^{-1} (\\theta))|.\n",
    "\\end{align}\n",
    "\n",
    "Derive a tractable optimization objective for identifying the optimal parameters $\\phi$ via gradient-based optimization assuming that $p_z = \\mathcal{N}(0, I)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 [40 Points]\n",
    "\n",
    "Recall HW2 Question 2, where you derived the conditional distribution of a Bayesian regression model with a likelihood $p(y\\lvert x, \\alpha, \\beta, \\gamma)$ corresponding to a linear observation model is $y = \\alpha x + \\beta + \\epsilon$. Noting that $\\gamma$ represents the noise level in the observed data, i.e. it corresponds to the precision of the data likelihood. In this case, the likelihood can be further expressed as:\n",
    "\n",
    "$p(y\\lvert x, \\alpha, \\beta, \\gamma) = (\\sqrt{\\frac{\\gamma}{2\\pi}})^n\\exp(-\\frac{\\gamma(\\sum_{i=1}^n\\|y_i - x_i\\alpha - \\beta\\|^2_2)}{2})$\n",
    "\n",
    "while the conditional posteriors are:\n",
    "\n",
    "$p(\\alpha\\lvert x, y, \\beta, \\gamma) = \\mathcal{N}(\\mu_1,\\frac{1}{\\lambda_1})$, where $\\mu_1 = \\frac{\\gamma \\sum_{i=1}^{n}x_i(y_i-\\beta)}{1+\\gamma \\sum_{i=1}^{n}x_i^2}$ and $\\lambda_1 = 1+\\gamma \\sum_{i=1}^{n}x_i^2$\n",
    "\n",
    "$p(\\beta\\lvert x, y, \\alpha, \\gamma) = \\mathcal{N}(\\mu_2,\\frac{1}{\\lambda_2})$ where $\\mu_2 = \\frac{\\gamma \\sum_{i=1}^{n}(y_i-x_i\\alpha)}{n\\gamma+1}$ and $\\lambda_2 = n\\gamma+1$\n",
    "\n",
    "$p(\\gamma\\lvert x, y, \\alpha, \\beta) =  \\text{Gam}(a,b)$ where $a=\\frac{n}{2}+2$ and $b = \\frac{1}{2}[2+\\sum_{i=1}^{n}(y_i-x_i\\alpha-\\beta)^2]$\n",
    "\n",
    "Generate your training data by considering true parameters $\\alpha, \\beta, \\gamma = 1.5, -3, 1$. Considering a uniform distribution, randomly sample $x$ in $[0, 3]$ using $N = 200$\n",
    "\n",
    "(1) Implement the Gibbs sampling algorithms using the above conditional posteriors to generate $5,000$ samples from the target posterior distribution $p(\\alpha,\\beta,\\gamma\\lvert x, y)$. Discard the first 2,000 as a burn-in phase and plot a histogram generated from your samples. Also, please draw some samples of linear lines using $50$ samples you got.\n",
    "\n",
    "(2) Gibbs sampling depends on deriving an analytical expression of the corresponding conditional posterior distributions. However, this cannot be done in many applications. Assume that you only have access to the unnormalized posterior, using the likelihood listed above and prior distributions: $p(\\alpha) = \\mathcal{N}(0, 1)$, $p(\\beta) = \\mathcal{N}(0, 1)$ and $p(\\gamma) = \\textrm{Gam(2, 1)}$. Implement the Metropolis algorithm to sample $5,000$ samples from the target posterior distribution $p(\\alpha,\\beta,\\gamma\\lvert x, y)$.  Discard the first 2,000 as a burn-in phase and plot a histogram generated from your samples. Please compare these histograms with the plots from Gibbs sampling. Also, please draw some samples of linear lines using $50$ samples you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 [20 Points]\n",
    "\n",
    "(1) The Fisher Information Matrix is defined as:\n",
    "$F = \\mathbb{E}_{p(x\\lvert \\theta)}[\\nabla_{\\theta}\\log p(x|\\theta)\\nabla_{\\theta}\\log p(x|\\theta)^T]$\n",
    "Prove that negative expected Hessian of log-likelihood is equal to the Fisher Information Matrix $F$.\n",
    "\n",
    "(2) Prove that Fisher Information Matrix F is the Hessian of KL-divergence between two distributions $p(x\\lvert \\theta)$ and $p(x\\lvert \\theta')$ with respect to $\\theta'$ evaluated at $\\theta' = \\theta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [20 Points]\n",
    "\n",
    "Consider the evolution of the Lotka-Volterra model (also called Predator-Prey equations) that describes the dynamics of two species $u$ and $v$ in a closed system. The dynamics of the system is governed by the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{du}{dt} &= \\alpha u - \\beta u v, \\\\\n",
    "    \\frac{dv}{dt} &= \\delta u v - \\gamma v,\n",
    "\\end{align}\n",
    "\n",
    "where, $u$ is the number of prey (for example, rabbits), $v$ is the number of predators (for example, foxes). The parameters $\\alpha, \\beta, \\gamma, \\delta$ are positive constants that describe the interaction between the two species. \n",
    "- $\\alpha$ is the growth rate of the prey population when there are no predators.\n",
    "- $\\beta$ is the rate at which predators kill prey.\n",
    "- $\\gamma$ is the rate at which predators die.\n",
    "- $\\delta$ is the rate at which predators reproduce.\n",
    "\n",
    "(1) Numerically integrate this system for choice of initial conditions $u(0) = 10, v(0) = 5$ and $\\alpha = 1, \\beta = 0.1, \\gamma = 1.5, \\delta = 0.75$. Plot the trajectories of $u$ and $v$ over a time of 15 days using 100 timesteps.\n",
    "\n",
    "(2) Perturb the numerically integrated data with Gaussian noise having 10% standard deviation of the clean data. Utilizing this data, infer the underlying parameters for the model utilizing [MALA](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm). Plot the posterior distributions for the inferred parameters. Consider the following priors for your model:\n",
    "\\begin{align}\n",
    "    p(\\alpha) &= \\mathrm{Gamma}(9, 0.1), \\\\\n",
    "    p(\\beta) &= \\mathrm{Gamma}(2, 0.05), \\\\\n",
    "    p(\\gamma) &= \\mathrm{Gamma}(2, 1), \\\\\n",
    "    p(\\delta) &= \\mathrm{Gamma}(1, 1), \\\\\n",
    "    p(\\sigma_n) &= \\mathrm{InvGamma}(1, 2)\n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuttingedge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4de6a45db83da0b758db7d08be5de2783b8b59abffac295330c3911b6a615d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Question 1 [15 Pts]:\n",
    "\n",
    "Suppose data $\\{x_i\\}_{i = 1}^{i = n}$ is drawn from a normal distribution of _known_ standard deviation $\\sigma$ and _unknown_ mean $\\mu$\n",
    "- Derive the MLE estimate $\\mu_{\\text{MLE}}$\n",
    "- Derive the MAP estimate $\\mu_{\\text{MAP}}$ assuming the prior is also a normal distribution with mean $\\tau$ and standard deviation $\\omega$. What happens as $n \\rightarrow \\infty$?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2 [30 Pts]:\n",
    "\n",
    "Consider a Bayesian linear regression model where the outputs $y$ are distributed according to a Gaussian likelihood $p(y\\lvert x, \\alpha, \\beta, \\gamma)$ corresponding to a linear model $y = \\alpha x + \\beta + \\epsilon$. Here $\\gamma$ represents the noise level in the observed data, i.e. it corresponds to the precision of the data likelihood. In this case, the likelihood can be further expressed as:\n",
    "\n",
    "$$p(y\\lvert x, \\alpha, \\beta, \\gamma) = (\\sqrt{\\frac{\\gamma}{2\\pi}})^n\\exp(-\\frac{\\gamma(\\sum_{i=1}^n\\|y_i - x_i\\alpha - \\beta\\|^2_2)}{2})$$\n",
    "\n",
    "In a Bayesian setting we would like to assume prior distributions on the unknown parameters $\\alpha$, $\\beta$ and $\\gamma$. Here we assume $p(\\alpha) = \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha})$, $p(\\beta) = \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta})$ and $p(\\gamma) = \\textrm{Gam}(\\tau, \\omega)$ is a Gamma distribution.\n",
    "\n",
    "Please write down the posterior conditional distribution for each of those parameters, i.e.,\n",
    "\n",
    "$p(\\alpha\\lvert x, y, \\beta, \\gamma)$\n",
    "\n",
    "$p(\\beta\\lvert x, y, \\alpha, \\gamma)$\n",
    "\n",
    "$p(\\gamma\\lvert x, y, \\alpha, \\beta)$\n",
    "\n",
    "Comment on your results.\n",
    "\n",
    "[**HINT**: Might be easier to work with the $\\log$ of the posterior, and \"complete the square\" to identify the distribution]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 [15 Pts]:\n",
    "\n",
    "Consider the Bayesian linear regression model with\n",
    "$$p(y\\mid x,w) = \\mathcal{N}(y\\mid w^{T}\\phi(x),\\alpha^{-1}I), \\ p(w)=\\mathcal{N}(w\\mid 0,\\beta^{-1}I),$$\n",
    "\n",
    "with $\\alpha = 0.5, \\beta  = 0.1$. Generate a set of $N = 400$ noisy observations by uniformly sampling\n",
    "\n",
    "$$y(x) = \\sin(\\pi x) + \\sin(2 \\pi x) + \\sin(5 \\pi x)\\quad x\\in[-1,1]$$\n",
    "\n",
    "Once you've created the observations, perturb the data with a normal distribution with standard deviation set to 10% of the data. Compute the MLE and MAP estimates for the weights $w$ using different types and numbers of features $\\phi(x)$:\n",
    "- Monomial basis: $\\phi(x) = \\{1, x, x^2, x^3, \\dots, x^M\\}$\n",
    "- Fourier basis: $\\phi(x) = \\{0, 1, \\sin(\\pi x), \\cos(\\pi x), \\sin(2\\pi x), \\cos(2\\pi x), \\dots, \\sin(M\\pi x), \\cos(M\\pi x)\\}$ (this case has a total of $2M$ features)\n",
    "- [Legendre](https://en.wikipedia.org/wiki/Legendre_polynomials) basis: $\\{P_0(x), P_1(x), P_2(x), P_3(x), \\dots, P_M(x)\\}$, where $P_0(x) = 1, P_1(x) = x$, and subsequent polynomials can be generated by the recursion $(n + 1) P_{n+1} (x) = (2n + 1) x P_n(x) - n P_{n - 1}(x).$\n",
    "\n",
    "For the case $M=5$, plot the data, the mean predictions corresponding to the MLE and MAP estimates for $w$, and $100$ samples from the predictive posterior distribution.\n",
    "Which set of features works best for this function and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 [20 Pts]:\n",
    "\n",
    "Write a logistic regression model to classify the letters from the [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset) letters dataset. Create an 80/20 train/test split of the data. You've been provided a template to carry this out. Fill in the suitable sections of the code.\n",
    "\n",
    "- First train your model to perform binary classification between the letters b and d. Visualize performance on the training and test sets using a confusion matrix.\n",
    "- Next, train your model to perform multiclass classification between b, d, p and q. Visualize performance on the training and test sets using a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.EMNIST(root='./', split='letters', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jnp.array(dataset.data)\n",
    "y = jnp.array(dataset.targets)\n",
    "# Fill in the code below to get letter b, d, p and q from the dataset\n",
    "\n",
    "\n",
    "# Combine them below to create datasets for usage in following parts\n",
    "# create X_bd, Y_bd for binary classification\n",
    "# create X_bdpq, Y_bdpq for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the mapping was performed correctly by utilizing a few examples:\n",
    "# Visualizing that your mapping is performed correctly\n",
    "rnd_idx = jax.random.randint(jax.random.PRNGKey(42), (16,), 0, X_bdpq.shape[0])\n",
    "# assuming that the labels are mapped as follows: b -> 0, d -> 1, p -> 2, q -> 3 (change if you're using something different)\n",
    "str_labels = ['b', 'd', 'p', 'q']\n",
    "pl.figure(dpi = 150, figsize=(12,12))\n",
    "for index, (image, label) in enumerate(zip(X_bdpq[rnd_idx], y_bdpq[rnd_idx])):\n",
    "    pl.subplot(4, 4, index + 1)\n",
    "    pl.imshow(jnp.reshape(image, (28,28)).T, cmap=pl.cm.gray)\n",
    "    pl.title('Label:' + str_labels[int(label)], fontsize = 20)\n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the code to perform an 80-20 train-test split on X_bd and y_bd to yield X_train, y_train, X_test, y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, X):\n",
    "    return jax.nn.sigmoid(X @ params)\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, X, y):\n",
    "    # fill this in to compute and return the binary cross entropy loss\n",
    "    return loss\n",
    "\n",
    "# Computing Gradients:\n",
    "grad_loss = jax.jit(jax.grad(loss_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights as zeros:\n",
    "params = jnp.zeros([784, 1])\n",
    "\n",
    "# Gradient descent\n",
    "# Learning Rate\n",
    "alpha = 5e-6\n",
    "n_its = 50000\n",
    "\n",
    "loss_history = [loss_fn(params, X_train, y_train)]\n",
    "for i in range(n_its):\n",
    "    g      = grad_loss(params, X_train, y_train)\n",
    "    params = jax.tree_map(lambda x, g: x - alpha * g, params, g)\n",
    "\n",
    "    # Track progress:\n",
    "    loss_history = loss_history + [loss_fn(params, X_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss history\n",
    "pl.figure(dpi = 150)\n",
    "pl.semilogy(loss_history)\n",
    "pl.ylabel('Loss')\n",
    "pl.xlabel('Number of Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Accuracy Score and Resulting Confusion Matrix for test set:\n",
    "accuracy_score = (jnp.round(forward(params, X_test)).ravel() == y_test).mean()\n",
    "cm             = metrics.confusion_matrix(y_test, jnp.round(forward(params, X_test)))\n",
    "cm_normalized  = cm.astype('float') / cm.sum(axis=1)[:, jnp.newaxis]\n",
    "\n",
    "print('Accuracy Score on Test Set:', accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(dpi = 150)\n",
    "sns.heatmap(cm_normalized, fmt = '4f', annot = True, square = True);\n",
    "pl.ylabel('Actual Label');\n",
    "pl.xlabel('Predicted Label');\n",
    "pl.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the code to perform an 80-20 train-test split on X_bdpq and y_bdpq to yield X_train, y_train, X_test, y_test:\n",
    "# Remember that you need to be one hot encoding the labels for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, X):\n",
    "    return jax.nn.softmax(X @ params)\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, X, y):\n",
    "    # fill this in to compute and return the cross entropy loss\n",
    "    return loss\n",
    "\n",
    "# Computing Gradients:\n",
    "grad_loss = jax.jit(jax.grad(loss_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights as zeros:\n",
    "params = jnp.zeros([784, 4])\n",
    "\n",
    "# Gradient descent\n",
    "# Learning Rate\n",
    "alpha = 1e-6\n",
    "n_its = 50000\n",
    "\n",
    "loss_history = [loss_fn(params, X_train, y_train)]\n",
    "for i in range(n_its):\n",
    "    g      = grad_loss(params, X_train, y_train)\n",
    "    params = jax.tree_map(lambda x, g: x - alpha * g, params, g)\n",
    "\n",
    "    # Track progress:\n",
    "    loss_history = loss_history + [loss_fn(params, X_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(dpi = 150)\n",
    "pl.semilogy(loss_history)\n",
    "pl.ylabel('Loss')\n",
    "pl.xlabel('Number of Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Accuracy Score and Resulting Confusion Matrix for test set:\n",
    "accuracy_score = (jnp.argmax(forward(params, X_test), axis = 1) == \n",
    "                  jnp.argmax(y_test, axis = 1)).mean()\n",
    "cm             = metrics.confusion_matrix(jnp.argmax(y_test, axis = 1), \n",
    "                                          jnp.argmax(forward(params, X_test), axis = 1))\n",
    "cm_normalized  = cm.astype('float') / cm.sum(axis=1)[:, jnp.newaxis]\n",
    "print('Accuracy Score on Test Set:', accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(dpi = 150)\n",
    "sns.heatmap(cm_normalized, fmt = '4f', annot = True, square = True);\n",
    "pl.ylabel('Actual Label');\n",
    "pl.xlabel('Predicted Label');\n",
    "pl.title('Confusion Matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 [20 Pts]:\n",
    "Write Python routines:\n",
    "- `gradient_descent(f, g, x0, eta)` that performs an iteration of gradient descent on a given function $f(x)$ and its gradient $g(x)$, starting at a given point $x_0$, and using a given step-size $\\eta$.\n",
    "- `newton(f, g, H, x0, eta)` that performs an iteration of Newton's method on a given function $f(x)$, its gradient $g(x)$ and Hessian $H(x)$, starting at a given point $x_0$, and using a given step-size $\\eta$.\n",
    "\n",
    "Do **NOT** use `jax.grad` or `jax.hessian` (you need to define the gradient and Hessian functions). Use those routines in an optimization loop to optimize the following two functions:\n",
    "\n",
    "- $$f_1(x, y) = x^2 + 100 y^2 $$\n",
    "- $$f_2(x, y) = (1 - x)^2 + 100(y - x^2)^2,$$\n",
    "\n",
    "Consider step-sizes $\\eta = 0.001, 0.01, 1$. Take your initial starting location as $(-1, 1)$. Comment on the following: \n",
    "- What is the effect of step-size on the different problems, and algorithms? \n",
    "- When the algorithm does converge, how many steps does it take to reach the minimum?\n",
    "\n",
    "Give suitable reasons for your observations. Plot the objective function value vs the iterate number, and a plot that shows the progress on a contour plot of the landscape. Use these to support your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuttingedge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4de6a45db83da0b758db7d08be5de2783b8b59abffac295330c3911b6a615d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
